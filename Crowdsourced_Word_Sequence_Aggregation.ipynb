{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crowdsourced Word Sequence Aggregation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApIyCfhnzPob",
        "colab_type": "text"
      },
      "source": [
        "#Data Helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPR1wqDd37jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = \"data/\"  # set your path of data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_6GqVGnid4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def loaddata(labelfilename,gtfilename):\n",
        "  gtdf = pd.read_csv(gtfilename,sep='\\t',encoding='utf-8')\n",
        "\n",
        "  sentences = gtdf['sentence'].values.tolist()\n",
        "  truelabels = {}\n",
        "  for idx,row in gtdf.iterrows():\n",
        "    truelabels[row['sentence']] = row['trueanswer'] \n",
        "\n",
        "  labeldf = pd.read_csv(labelfilename,sep='\\t',encoding='utf-8')\n",
        "\n",
        "  workers = []\n",
        "  swlabels = []\n",
        "  \n",
        "  for idx,row in labeldf.iterrows():\n",
        "    worker = row['worker']\n",
        "    if worker not in workers:\n",
        "      workers.append(worker)\n",
        "    workerid = workers.index(worker)\n",
        "    sentenceid = sentences.index(row['sentence'])\n",
        "    label = row['workeranswer'].strip()\n",
        "    swlabels.append((sentenceid,workerid,label))\n",
        "  \n",
        "  return (workers,sentences,swlabels,truelabels)\n",
        "\n",
        "def labelformatconversion(workers,sentences,swlabels):\n",
        "  wlabelidlists = {}\n",
        "  wsentenceidlists = {}\n",
        "  for worker in workers:\n",
        "    wlabelidlists[worker] = []\n",
        "    wsentenceidlists[worker] = []\n",
        "    \n",
        "  slabelidlists = {}\n",
        "  sworkeridlists = {}\n",
        "  for sentence in sentences:\n",
        "    slabelidlists[sentence] = []\n",
        "    sworkeridlists[sentence] = []\n",
        "    \n",
        "  labellist = []\n",
        "  labelidx = 0\n",
        "  for (sentenceid,workerid,label) in swlabels:\n",
        "    labellist.append(label)\n",
        "    sentence = sentences[sentenceid]\n",
        "    slabelidlists[sentence].append(labelidx)\n",
        "    sworkeridlists[sentence].append(workerid)\n",
        "    worker = workers[workerid]\n",
        "    wlabelidlists[worker].append(labelidx)\n",
        "    wsentenceidlists[worker].append(sentenceid)\n",
        "    labelidx += 1\n",
        "  \n",
        "  return (labellist,slabelidlists,sworkeridlists,wlabelidlists,wsentenceidlists)\n",
        "\n",
        "def truelabelformatonversion(sentences,truelabels):\n",
        "  truelabellist = []\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    truelabellist.append(truelabels[sentence])\n",
        "    \n",
        "  return truelabellist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPS2voaTz2yA",
        "colab_type": "text"
      },
      "source": [
        "# Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFUa2_Yp_Hov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pVpk5Jg_J6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTgWfpMU_NDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0SgMy6k_rvV",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEtDUi895wlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_SIM_TYPE = 'COSINE'\n",
        "def similarity(embed1, embed2, simtype = DEFAULT_SIM_TYPE):\n",
        "  if (simtype == 'COSINE'):\n",
        "    l1 = np.sqrt(np.sum(embed1**2))\n",
        "    l2 = np.sqrt(np.sum(embed2**2))\n",
        "    sim = np.inner(embed1,embed2) / (l1*l2)\n",
        "  \n",
        "  return sim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lAPMiUblQB9",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqJP7lGCcXoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluationbyEmbedding(sentences, elabels, label_embeddings, truelabels, truelabel_embeddings):\n",
        "  totalsim = 0\n",
        "  for sentence in elabels:\n",
        "    elabel_embedding = label_embeddings[elabels[sentence]]\n",
        "    truelabel_embedding = truelabel_embeddings[sentences.index(sentence)]\n",
        "    totalsim += similarity(elabel_embedding,truelabel_embedding)\n",
        "    \n",
        "  #print(len(elabels),totalsim/len(elabels))\n",
        "  return totalsim/len(elabels)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edlrVNvtlRPF",
        "colab_type": "text"
      },
      "source": [
        "## GLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFqsU43l0OF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/gcunhase/NLPMetrics\n",
        "#https://colab.research.google.com/github/gcunhase/NLPMetrics/blob/master/notebooks/gleu.ipynb\n",
        "\n",
        "import nltk\n",
        "import nltk.translate.gleu_score as gleu\n",
        "\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')\n",
        "\n",
        "def evaluationbyGLEU(sentences, labels, elabelidxs, truelabels):\n",
        "  n = len(sentences)\n",
        "  sim = 0\n",
        "  for i in range(n):\n",
        "    sentence = sentences[i]\n",
        "    labelidx = elabelidxs[sentence]\n",
        "    label = labels[labelidx]\n",
        "    truelabel = truelabels[sentence]\n",
        "    sim += gleu.sentence_gleu([truelabel.split()], label.split())\n",
        "  #print(n,sim/n)\n",
        "  return sim/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0e0la_aHALA",
        "colab_type": "text"
      },
      "source": [
        "#Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7kXVGP1gwtX",
        "colab_type": "text"
      },
      "source": [
        "## SMV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p7TajypN6dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SMVSelectionAndEvaluationbyEmbedding(sentences, slabelidlists, label_embeddings, truelabel_embeddings):\n",
        "  total_sim = 0\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    truelabel_embedding = truelabel_embeddings[sentences.index(sentence)]\n",
        "    slabel_embeddings = label_embeddings[slabelidlists[sentence]]\n",
        "    onetotal_sim = 0\n",
        "    for label_embedding in slabel_embeddings:\n",
        "      onetotal_sim += similarity(label_embedding,truelabel_embedding)\n",
        "    total_sim += onetotal_sim/len(slabel_embeddings)\n",
        "    \n",
        "  #print(len(sentences),total_sim/len(sentences))\n",
        "  return total_sim/len(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKwcTFl60jvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SMVSelectionAndEvaluationbyGLEU(sentences, slabelidlists, labels, truelabels):\n",
        "  total_sim = 0\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    truelabel = truelabels[sentences.index(sentence)]\n",
        "    slabels = labels[slabelidlists[sentence]]\n",
        "    onetotal_sim = 0\n",
        "    for label in slabels:\n",
        "      onetotal_sim += gleu.sentence_gleu([truelabel.split()], label.split())\n",
        "    total_sim += onetotal_sim/len(slabels)\n",
        "    \n",
        "  #print(len(sentences),total_sim/len(sentences))\n",
        "  return total_sim/len(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ctiz2pgmfY",
        "colab_type": "text"
      },
      "source": [
        "##SMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKVpqBD-HIcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SMSSelectionOne(labelidlist, slabel_embeddings):\n",
        "  labelnum = len(slabel_embeddings)\n",
        "  simmat = np.zeros((labelnum,labelnum))\n",
        "  for i in range(labelnum):\n",
        "    for j in range(labelnum):\n",
        "      simmat[i][j] = similarity(slabel_embeddings[i],slabel_embeddings[j])\n",
        "      \n",
        "  sim = np.sum(simmat,axis=1)\n",
        "  maxlabelidx = np.argmax(sim)\n",
        "  return labelidlist[maxlabelidx]\n",
        "\n",
        "def SMSSelection(sentences, slabelidlists, label_embeddings):\n",
        "  elabels = {}\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    slabel_embeddings = label_embeddings[slabelidlists[sentence]]\n",
        "    elabel = SMSSelectionOne(slabelidlists[sentence], slabel_embeddings)\n",
        "    elabels[sentence] = elabel\n",
        "  return elabels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHcDtaxxiHEi",
        "colab_type": "text"
      },
      "source": [
        "## RASA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj9wdUzHjcrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import chi2\n",
        "\n",
        "def RASAInference(workers, sentences, slabelidlists, sworkeridlists, wlabelidlists, wsentenceidlists, label_embeddings):\n",
        "  max_ite = 1000\n",
        "  \n",
        "  # initilizing the estimated embedding by using SMV\n",
        "  estimated_embeddings = []\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    slabel_embeddings = label_embeddings[slabelidlists[sentence]]\n",
        "    estimated_embeddings.append(np.mean(slabel_embeddings, axis=0))\n",
        "  estimated_embeddings = np.asarray(estimated_embeddings)\n",
        "  \n",
        "  w_num = len(workers)\n",
        "  reliability = np.zeros(w_num)\n",
        "  s_num = len(sentences)\n",
        "  \n",
        "  ite = 0\n",
        "  eb_change = 1\n",
        "  while (ite <= max_ite) and (eb_change > 0.0000001):\n",
        "    old_estimated_embeddings = np.copy(estimated_embeddings)\n",
        "    \n",
        "    # compute reliability\n",
        "    for j in range(w_num): \n",
        "      worker = workers[j]\n",
        "      westimated_embeddings = estimated_embeddings[wsentenceidlists[worker]]\n",
        "      nw = len(wsentenceidlists[worker])\n",
        "      chiw = chi2.isf(q=0.025, df=nw)\n",
        "      wlabelidlist = wlabelidlists[worker]\n",
        "      wlabel_embeddings = label_embeddings[wlabelidlist]\n",
        "      diff_embedding = (westimated_embeddings - wlabel_embeddings)\n",
        "      if (np.sum(diff_embedding) == 0):\n",
        "        reliability[j] = 1\n",
        "      else: \n",
        "        reliability[j] = chiw / np.sum(diff_embedding*diff_embedding)\n",
        "    \n",
        "    # compute estimated embeddings\n",
        "    for i in range(s_num):\n",
        "      sentence = sentences[i]\n",
        "      slabelidlist = slabelidlists[sentence]\n",
        "      slabel_embeddings = label_embeddings[slabelidlist]\n",
        "      sworkeridlist = sworkeridlists[sentence]\n",
        "      sworkeridrelaiblity = reliability[sworkeridlist]\n",
        "      for j in range(len(sworkeridrelaiblity)):\n",
        "        slabel_embeddings[j] = slabel_embeddings[j] * sworkeridrelaiblity[j]\n",
        "      estimated_embeddings[i] = np.sum(slabel_embeddings, axis=0) / np.sum(sworkeridrelaiblity)\n",
        "      \n",
        "    eb_diff = (estimated_embeddings - old_estimated_embeddings)\n",
        "    eb_change = np.sum(eb_diff * eb_diff)\n",
        "    ite += 1\n",
        "  \n",
        "  return (estimated_embeddings,reliability)\n",
        "\n",
        "def RASASelectionOne(estimated_embedding, labelidlist, label_embeddings):\n",
        "  slabel_num = len(labelidlist)\n",
        "  slabel_embeddings = label_embeddings[labelidlist]\n",
        "  simvec = np.zeros(slabel_num)\n",
        "  for k in range(slabel_num):\n",
        "    simvec[k] = similarity(estimated_embedding,slabel_embeddings[k])\n",
        "  maxlabelidx = np.argmax(simvec)\n",
        "  return labelidlist[maxlabelidx]\n",
        "\n",
        "def RASASelection(sentences, slabelidlists, label_embeddings, estimated_embeddings):\n",
        "  elabels = {}\n",
        "  for i in range(len(sentences)):\n",
        "      sentence = sentences[i]\n",
        "      slabelidlist = slabelidlists[sentence]\n",
        "      elabel = RASASelectionOne(estimated_embeddings[i], slabelidlist, label_embeddings)\n",
        "      elabels[sentence] = elabel\n",
        "  return elabels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMV22LusVH4M",
        "colab_type": "text"
      },
      "source": [
        "##Optimal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2_oU8BaVDUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OptimalSelectionAndEvaluationbyEmbedding(sentences, slabelidlists, label_embeddings, truelabel_embeddings):\n",
        "  total_sim = 0\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    truelabel_embedding = truelabel_embeddings[sentences.index(sentence)]\n",
        "    slabel_embeddings = label_embeddings[slabelidlists[sentence]]\n",
        "    ns = len(slabel_embeddings)\n",
        "    sims = np.zeros(ns)\n",
        "    for k in range(ns):\n",
        "      label_embedding = slabel_embeddings[k]\n",
        "      sims[k] = similarity(label_embedding,truelabel_embedding)\n",
        "    maxsim = np.max(sims)\n",
        "    total_sim += maxsim\n",
        "  #print(len(sentences),total_sim/len(sentences))\n",
        "  return total_sim/len(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqIIeTbz0Xdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OptimalSelectionAndEvaluationbyGLEU(sentences, slabelidlists, labels, truelabels):\n",
        "  total_sim = 0\n",
        "  for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    truelabel = truelabels[sentences.index(sentence)]\n",
        "    slabels = labels[slabelidlists[sentence]]\n",
        "    ns = len(slabels)\n",
        "    sims = np.zeros(ns)\n",
        "    for k in range(ns):\n",
        "      label = slabels[k]\n",
        "      sims[k] = gleu.sentence_gleu([truelabel.split()], label.split())\n",
        "    maxsim = np.max(sims)\n",
        "    total_sim += maxsim\n",
        "  #print(len(sentences),total_sim/len(sentences))\n",
        "  return total_sim/len(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4JmNL1Z_Ubj",
        "colab_type": "text"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_5NHHXbx_kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(labelfilename,gtfilename):\n",
        "  # labellist: all labels for all sentences by all workers\n",
        "  # slabelidlists: label id list for each sentence\n",
        "  # sworkeridlists: worker id list for each sentence\n",
        "  # wlabelidlists: label id list for each worker\n",
        "  # wsentenceidlists: sentence id list for each worker\n",
        "  (workers,sentences,swlabels,truelabels) = loaddata(labelfilename,gtfilename)\n",
        "  (labellist,slabelidlists,sworkeridlists,wlabelidlists,wsentenceidlists) = labelformatconversion(workers,sentences,swlabels)\n",
        "  (truelabellist) = truelabelformatonversion(sentences,truelabels)\n",
        "\n",
        "  # label_embeddings: embeddings of all labels of workers\n",
        "  # truelabel_embeddings: embeddings of all true labels\n",
        "\n",
        "  # Import the Universal Sentence Encoder's TF Hub module\n",
        "  embed = hub.Module(module_url)\n",
        "\n",
        "  # Reduce logging output.\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    label_embeddings = session.run(embed(labellist))\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    truelabel_embeddings = session.run(embed(truelabellist))\n",
        "\n",
        "  # print('Optimal')\n",
        "  perf_opt_embedding = OptimalSelectionAndEvaluationbyEmbedding(sentences, slabelidlists, label_embeddings, truelabel_embeddings)\n",
        "  perf_opt_gleu = OptimalSelectionAndEvaluationbyGLEU(sentences, slabelidlists, np.asarray(labellist), np.asarray(truelabellist)) \n",
        "\n",
        "  # print('SMV')\n",
        "  perf_smv_embedding = SMVSelectionAndEvaluationbyEmbedding(sentences, slabelidlists, label_embeddings, truelabel_embeddings)\n",
        "  perf_smv_gleu = SMVSelectionAndEvaluationbyGLEU(sentences, slabelidlists, np.asarray(labellist), np.asarray(truelabellist))\n",
        "\n",
        "\n",
        "  # print('SMS')\n",
        "  LSNNelabels = SMSSelection(sentences, slabelidlists, label_embeddings)\n",
        "  perf_sms_embedding = evaluationbyEmbedding(sentences, LSNNelabels, label_embeddings, truelabels, truelabel_embeddings)\n",
        "  perf_sms_gleu = evaluationbyGLEU(sentences, np.asarray(labellist), LSNNelabels, truelabels)\n",
        "\n",
        "  # print('RASA')\n",
        "  (estimated_embeddings, reliability) = RASAInference(workers, sentences, slabelidlists, sworkeridlists, wlabelidlists, wsentenceidlists, label_embeddings)\n",
        "  LSCATDelabels = RASASelection(sentences, slabelidlists, label_embeddings, estimated_embeddings)\n",
        "  perf_rasa_embedding = evaluationbyEmbedding(sentences, LSCATDelabels, label_embeddings, truelabels, truelabel_embeddings)\n",
        "  perf_rasa_gleu = evaluationbyGLEU(sentences, np.asarray(labellist), LSCATDelabels, truelabels)\n",
        "\n",
        "  print(\"Evaluation,SMV,SMS,RASA,Optimal\")\n",
        "  print(\"Embedding,%.4f,%.4f,%.4f,%.4f\" % (perf_smv_embedding,perf_sms_embedding,perf_rasa_embedding,perf_opt_embedding))\n",
        "  print(\"GLEU,%.4f,%.4f,%.4f,%.4f\" % (perf_smv_gleu,perf_sms_gleu,perf_rasa_gleu,perf_opt_gleu))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FGOdBg2bUQo",
        "colab_type": "code",
        "outputId": "83c1807e-f3b0-487d-f970-c6333e310a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# J1\n",
        "labelfilename = folder + 'CrowdWSA2019_J1_label_anonymous.tsv'\n",
        "gtfilename = folder + 'CrowdWSA2019_J1_gt.tsv'\n",
        "\n",
        "print(\"Data:J1\")\n",
        "run(labelfilename,gtfilename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data:J1\n",
            "Evaluation,SMV,SMS,RASA,Optimal\n",
            "Embedding,0.7354,0.7969,0.7914,0.8853\n",
            "GLEU,0.1930,0.2627,0.2519,0.4990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zLU62tscHdb",
        "colab_type": "code",
        "outputId": "0124935c-c01b-4046-82ff-7335607513fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# T1\n",
        "labelfilename = folder + 'CrowdWSA2019_T1_label_anonymous.tsv'\n",
        "gtfilename = folder + 'CrowdWSA2019_T1_gt.tsv'\n",
        "\n",
        "print(\"Data:T1\")\n",
        "run(labelfilename,gtfilename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data:T1\n",
            "Evaluation,SMV,SMS,RASA,Optimal\n",
            "Embedding,0.7851,0.8377,0.8451,0.9047\n",
            "GLEU,0.1740,0.2194,0.2296,0.3698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64j21fhKcVII",
        "colab_type": "code",
        "outputId": "5dc4abde-4087-4ee2-ab31-8c1aad1ce55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# T2\n",
        "labelfilename = folder + 'CrowdWSA2019_T2_label_anonymous.tsv'\n",
        "gtfilename = folder + 'CrowdWSA2019_T2_gt.tsv'\n",
        "\n",
        "print(\"Data:T2\")\n",
        "run(labelfilename,gtfilename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data:T2\n",
            "Evaluation,SMV,SMS,RASA,Optimal\n",
            "Embedding,0.7696,0.8288,0.8339,0.8986\n",
            "GLEU,0.1616,0.2170,0.2345,0.3637\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}